<div id="readability-page-1" class="page"><div>
			
<p>We’ve all watched Iron Man and quietly envied Tony Stark’s banter with JARVIS. There is something profoundly magical about speaking to a machine and having it understand, think, and reply, not with a pre-programmed script, but with genuine intelligence. For a long time, building something like that required a PhD or massive cloud subscriptions. Today, we can make it in an afternoon with a few lines of Python. In this article, we will build a voice AI assistant that listens to you, thinks using a powerful local AI model (Llama 3), and responds to you.</p>



<h2>How a Voice AI Assistant Works in Real-time?</h2>



<p>Before we write the code, let’s understand what we are building. An AI voice assistant is essentially a loop of three distinct biological functions replicated by code:</p>



<ol>
<li><strong>The Ears (Speech-to-Text):</strong> We capture audio vibrations and translate them into text.</li>



<li><strong>The Brain (LLM Inference):</strong> We send that text to a Large Language Model (Ollama/Llama 3) to generate a smart response.</li>



<li><strong>The Mouth (Text-to-Speech):</strong> We convert the AI’s text response back into audio so we can hear it.</li>
</ol>



<p>Let’s understand it practically by building a real-time voice AI assistant using Python.</p>







<p>To get it running, we are relying on three key libraries. You will need to install them via your terminal:</p>



<pre><strong>pip install speechrecognition ollama pyttsx3 pyaudio</strong></pre>



<p><strong>You must have the Ollama application installed on your computer and the Llama 3 model pulled (ollama pull llama3) for the brain part of our code to work. Here’s a <a href="https://amanxai.com/2025/11/15/run-a-powerful-llm-locally-on-your-laptop/" rel="noreferrer noopener">tutorial</a> if you are a first timer.</strong></p>



<h4>Step 1: Importing the Tools</h4>



<p>Here we are grabbing our tools. Think of this as laying out your ingredients before cooking. <strong>sr</strong> is our listener, <strong>ollama</strong> is our thinker, and <strong>pyttsx3</strong> is our speaker:</p>







<h4>Step 2: The Ears</h4>



<p>This function is responsible for the physical world interface, the microphone:</p>







<p>Here’s what’s happening:</p>



<ol>
<li><strong>adjust_for_ambient_noise</strong>: Microphones pick up fan hums and static. This line tells the code to listen to the silence for 0.5 seconds to understand the room’s baseline noise, which makes the actual recognition much more accurate.</li>



<li><strong>recognize_google</strong>: We are using Google’s Web Speech API to convert audio to text. It’s free and generally very accurate, though it does require an internet connection.</li>
</ol>



<h4>Step 3: The Brain</h4>



<p>This is the key part of our assistant. We take the raw text and give it intelligence:</p>







<p>Here’s what’s happening:</p>



<ol>
<li><strong>ollama.chat</strong>: This is the interface to your local Llama 3 model. We send a list of messages (in this case, just one from the “user”) and wait for the model to complete the pattern.</li>
</ol>



<ul>
<li><strong>Latency</strong>: Since Llama 3 is running locally on your device, this might take a second or two, depending on your GPU/CPU, but it’s completely private. No data is sent to a cloud server for thinking.</li>
</ul>



<h4>Step 4: The Mouth</h4>



<p>An assistant isn’t an assistant if you have to read the screen. Here’s how to give it the ability to speak:</p>







<p>Here’s what’s happening:</p>



<ol>
<li><strong>pyttsx3.init()</strong>: This initialises the speech engine driver on your OS (sapi5 on Windows, nsss on Mac, espeak on Linux).</li>



<li><strong>engine.runAndWait()</strong>: This is critical. It blocks the code execution until the speaking is done. Without this, the program might try to listen while it’s still speaking, causing it to hear itself!</li>
</ol>



<h4>Step 5: The Main Function</h4>



<p>Finally, we stitch the organs together into a living body:</p>







<p>Here’s what’s happening:</p>



<ol>
<li><strong>The while True Loop</strong>: This creates the always-on behaviour. The program enters a cycle of Listen -&gt; Think -&gt; Speak, and then immediately goes back to Listen.</li>



<li><strong>Exit Strategy</strong>: We added a simple check for exit or stop so you can gracefully shut down the assistant without force-quitting the terminal.</li>
</ol>



<p>Here’s the output with an example:</p>



<figure><img data-recalc-dims="1" decoding="async" width="1024" height="509" data-attachment-id="28702" data-permalink="https://amanxai.com/2025/12/02/build-a-real-time-voice-ai-assistant/image-258/" data-orig-file="https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?fit=1788%2C888&amp;ssl=1" data-orig-size="1788,888" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?fit=300%2C149&amp;ssl=1" data-large-file="https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?fit=1024%2C509&amp;ssl=1" src="https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=1024%2C509&amp;ssl=1" alt="Real-Time Voice AI Assistant: output" srcset="https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=1024%2C509&amp;ssl=1 1024w, https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=300%2C149&amp;ssl=1 300w, https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=768%2C381&amp;ssl=1 768w, https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=1536%2C763&amp;ssl=1 1536w, https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?resize=1200%2C596&amp;ssl=1 1200w, https://i0.wp.com/amanxai.com/wp-content/uploads/2025/12/image.png?w=1788&amp;ssl=1 1788w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h3>Closing Thoughts</h3>



<p>When you run this script and hear the AI respond to your voice, take a moment to appreciate what just happened. You essentially built a synthetic neocortex (Llama 3) and gave it sensory organs (mic/speakers).</p>



<p>Today it’s just chatting; tomorrow, you could hook the think() function up to your calendar API or email client, turning this from a chatbot into a true proactive agent.</p>



<p>I hope you liked this article on building a voice AI assistant that listens to you, thinks using a powerful local AI model, and speaks back to you. <strong>Follow me on <a href="https://www.instagram.com/amankharwal.official/" rel="noreferrer noopener">Instagram</a> for many more resources</strong>.</p>

		</div></div>